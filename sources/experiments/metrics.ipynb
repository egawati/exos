{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c7ca91-1ab8-451a-8a1e-6b2a8d713259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e978f08-77c1-4630-9728-22b2d76ab9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exos_running_time_df(results):\n",
    "    df_dict = {}\n",
    "    output = results['output']\n",
    "    windows = output.keys()\n",
    "    neigh_times = list()\n",
    "    est_times = list()\n",
    "    out_attrs_times = list()\n",
    "    for window in windows:\n",
    "        est_times.append(output[window]['est_time'])\n",
    "        del output[window]['est_time']\n",
    "        neigh_time = max([output[window][stream_id]['temporal_neighbor_time'] for stream_id in output[window].keys()])\n",
    "        neigh_times.append(neigh_time)\n",
    "        out_attrs_time = max([output[window][stream_id]['out_attrs_time'] for stream_id in output[window].keys()])\n",
    "        out_attrs_times.append(out_attrs_time)\n",
    "    df_dict['windows'] = windows\n",
    "    df_dict['est_times'] = est_times\n",
    "    df_dict['neigh_times'] = neigh_times\n",
    "    df_dict['out_attrs_times'] = out_attrs_times\n",
    "    return pd.DataFrame.from_dict(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266371eb-098e-472e-b4e8-1a1fd7901c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickled_results(filename):\n",
    "    exos_file = open(filename, 'rb')\n",
    "    results = pickle.load(exos_file)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6408a56-f548-40de-91c0-c1ca5ebc4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_attribute_based_on_contribution_score(outlying_attributes, ground_truth):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ------\n",
    "    outlying_attributes: dict\n",
    "        key-value pair, key=attribute's name, value=attribute's contribution score\n",
    "    ground_truth : list\n",
    "        list of outlying attributes\n",
    "    outlying attributes are ordered by their contribution scores from the highest to the lowest\n",
    "    \"\"\"\n",
    "    high_score_attr = list(outlying_attributes.keys())[0]\n",
    "    if high_score_attr in ground_truth:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e023681-e51f-4bd2-9414-cbbbc7e216cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(gt_folder, gt_filename, result_folder, result_filename,\n",
    "                     n_streams, window_size, score_precision = 0.1):\n",
    "    result_path = f'{result_folder}/{result_filename}'\n",
    "    results = unpickled_results(result_path)\n",
    "    windows = tuple(results['output'].keys()) ## get tuple of window ids : (window_0, window_1, ...)\n",
    "    n_outliers = 0\n",
    "    matched = list()\n",
    "    matched2 = list()\n",
    "    for i in range(n_streams):\n",
    "        gt_path = f'{gt_folder}/{i}_{gt_filename}' #ground truth filepath\n",
    "        df = pd.read_pickle(gt_path)\n",
    "        df = df[['label', 'outlying_attributes']]\n",
    "        for j, window in enumerate(windows):\n",
    "            outlier_indices = results['output'][window][i]['outlier_indices']\n",
    "            if outlier_indices is not None:\n",
    "                outlier_indices = outlier_indices[i]\n",
    "                new_df = df.iloc[j*window_size:(j+1)*window_size].reset_index(drop=True)\n",
    "                n_outliers += len(outlier_indices)\n",
    "                ground_truth = new_df.iloc[outlier_indices].reset_index(drop=True)\n",
    "                outlying_attributes = results['output'][window][i]['out_attrs']\n",
    "                for idx , gt in ground_truth.iterrows():\n",
    "                    check = match_attribute_based_on_contribution_score(outlying_attributes[idx],\n",
    "                                                                   gt['outlying_attributes'])\n",
    "                    matched.append(check)\n",
    "    matched = [True for item in matched if item == True]\n",
    "    return matched, n_outliers, results['simulator_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd809d5-b73d-453f-ae09-3d8551ce189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nstreams</th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>avg_running_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>73.442000</td>\n",
       "      <td>36.022518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>76.211000</td>\n",
       "      <td>57.036936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>79.982667</td>\n",
       "      <td>77.151869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>81.844000</td>\n",
       "      <td>108.403863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>81.947600</td>\n",
       "      <td>132.502229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>82.998667</td>\n",
       "      <td>161.924292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nstreams  avg_accuracy  avg_running_time\n",
       "0         5     73.442000         36.022518\n",
       "1        10     76.211000         57.036936\n",
       "2        15     79.982667         77.151869\n",
       "3        20     81.844000        108.403863\n",
       "4        25     81.947600        132.502229\n",
       "5        30     82.998667        161.924292"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recap_performance_info(rel_path =  'pickles/nstreams', \n",
    "                           n_streams=(5,10,15),\n",
    "                           bname = 'k1_100K_Case1'):\n",
    "    cwd = os.getcwd()\n",
    "    avg_accuracies = list()\n",
    "    avg_running_time = list()\n",
    "    streams = list()\n",
    "    for nstreams in n_streams:\n",
    "        file_path = f'{rel_path}/{nstreams}/performance_{nstreams}_{bname}.pkl'\n",
    "        path = os.path.join(cwd, file_path)\n",
    "        df = pd.read_pickle(path)\n",
    "        mean_accuracy = df['accuracy'].mean()\n",
    "        avg_accuracies.append(mean_accuracy)\n",
    "        mean_running_time = df['running_time'].mean()\n",
    "        avg_running_time.append(mean_running_time)\n",
    "        streams.append(nstreams)\n",
    "    performance = {'nstreams' : streams, \n",
    "                   'avg_accuracy' : avg_accuracies,\n",
    "                   'avg_running_time' : avg_running_time}\n",
    "    df = pd.DataFrame(performance)\n",
    "    df.to_pickle(f'{rel_path}/avg_performance_{bname}.pkl')\n",
    "    return df\n",
    "recap_performance_info(rel_path  =  'pickles/nstreams', \n",
    "                       n_streams = (5,10,15,20,25,30),\n",
    "                       bname = 'k1_100K_Case1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c06e9b8-8b30-4228-a5f8-bf58614a12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(out_attrs, ground_truth, total_attributes):\n",
    "    TP = len(set(out_attrs) & set(ground_truth))\n",
    "    FP = len(set(out_attrs) - set(ground_truth))\n",
    "    FN = len(set(ground_truth) - set(out_attrs))\n",
    "    TN = total_attributes - (TP + FP + FN)\n",
    "    confusion_matrix = {'TP' : TP,\n",
    "                        'FP' : FP,\n",
    "                        'FN' : FN,\n",
    "                        'TN' : TN}\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_precision(confusion_matrix):\n",
    "    precision = confusion_matrix['TP'] / ( confusion_matrix['TP'] +  confusion_matrix['FP'])\n",
    "    return precision\n",
    "\n",
    "def compute_recall(confusion_matrix):\n",
    "    recall = confusion_matrix['TP'] / ( confusion_matrix['TP'] +  confusion_matrix['FN'])\n",
    "    return recall\n",
    "\n",
    "def compute_f1_score(precision, recall):\n",
    "    if precision+recall == 0:\n",
    "        return 0\n",
    "    f1_score = (2 * precision * recall ) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23658cce-dfcd-4ee9-9f9a-397bf57b4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_v2(gt_folder, gt_filename, result_folder, result_filename,\n",
    "                           n_streams, window_size, non_data_attr=2):\n",
    "    \"\"\"\n",
    "    compute TP, FP, TN and FN and then compute precision, recall and F1 score\n",
    "    \"\"\"\n",
    "    result_path = f'{result_folder}/{result_filename}'\n",
    "    results = unpickled_results(result_path)\n",
    "    windows = tuple(results['output'].keys()) ## get tuple of window ids : (window_0, window_1, ...)\n",
    "    n_outliers = 0\n",
    "    accuracies = {}\n",
    "    for i in range(n_streams):\n",
    "        precision_list = list()\n",
    "        recall_list = list()\n",
    "        f1_score_list = list()\n",
    "        gt_path = f'{gt_folder}/{i}_{gt_filename}' #ground truth filepath\n",
    "        df = pd.read_pickle(gt_path)\n",
    "        n_attributes = df.shape[1] - non_data_attr\n",
    "        df = df[['label', 'outlying_attributes']]\n",
    "        for j, window in enumerate(windows):\n",
    "            outlier_indices = results['output'][window][i]['outlier_indices']\n",
    "            if outlier_indices is not None:\n",
    "                outlier_indices = outlier_indices[i]\n",
    "                new_df = df.iloc[j*window_size:(j+1)*window_size].reset_index(drop=True)\n",
    "                n_outliers += len(outlier_indices)\n",
    "                ground_truth = new_df.iloc[outlier_indices].reset_index(drop=True)\n",
    "                outlying_attributes = results['output'][window][i]['out_attrs']\n",
    "                for idx , gt in ground_truth.iterrows():\n",
    "                    confusion_matrix = get_confusion_matrix(outlying_attributes[idx], \n",
    "                                                            gt['outlying_attributes'], \n",
    "                                                            n_attributes)\n",
    "                    precision = compute_precision(confusion_matrix)\n",
    "                    recall = compute_recall(confusion_matrix)\n",
    "                    f1_score = compute_f1_score(precision, recall)\n",
    "                    precision_list.append(precision)\n",
    "                    recall_list.append(recall)\n",
    "                    f1_score_list.append(f1_score)\n",
    "        accuracies[i] = {'precision' : precision_list,\n",
    "                         'recall' : recall_list,\n",
    "                         'f1_score' : f1_score_list,}\n",
    "    return n_outliers, accuracies, results['simulator_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6786f525-cbc1-4bf6-9df7-dc82c153d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_performance(gt_folder, gt_filename, result_folder, result_filename,\n",
    "                          performance_folder,\n",
    "                          n_streams, window_size, non_data_attr=2):\n",
    "    n_outliers, accuracies, simulation_time = compute_performance_v2(gt_folder, \n",
    "                                                                     gt_filename, \n",
    "                                                                     result_folder, \n",
    "                                                                     result_filename,\n",
    "                                                                     n_streams, \n",
    "                                                                     window_size, \n",
    "                                                                     non_data_attr)\n",
    "    df= pd.DataFrame(accuracies[0])\n",
    "    for i in range(1, n_streams):\n",
    "        ndf = pd.DataFrame(accuracies[i])\n",
    "        df = df.append(ndf, ignore_index = True)\n",
    "    ### for sanity checking\n",
    "    if n_outliers == df.shape[0]:\n",
    "        df.to_pickle(f\"{performance_folder}/{result_filename}\")\n",
    "    return df, simulation_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
